{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS 667 - Fall 2016 Project\n",
    "\n",
    "This notebook is used to demonstrate some examples of using a pre-trained convolutional neural network to produce new images based on what the neural net believes it sees in the input image.\n",
    "\n",
    "[Caffe](http://caffe.berkeleyvision.org/) is the neural network framework that is used, and the code is adapted from the Google [DeepDream](https://github.com/google/deepdream) project. DeepDream uses the GoogleNet network model developed by Google, originally developed for the ImageNet Large-Scale Visual Recognition Challenge in 2014.\n",
    "\n",
    "***\n",
    "\n",
    "The DeepDream GoogleNet contains many layers as seen in this [graph](googlenet_deploy.jpeg) created by the Caffe `draw_net.py` script.\n",
    "\n",
    "![DeepDream network](googlenet_deploy.jpeg)\n",
    "\n",
    "***\n",
    "\n",
    "The code in this notebook is licensed under the Apache License 2.0, which allows the use, modification and redistribution of the code. See [LICENSE](LICENSE) for details.\n",
    "\n",
    "## Code\n",
    "\n",
    "These functions are used throughout the notebook to process the various image files.\n",
    "\n",
    "The `showarray` function is used to display the image inline in the notebook.\n",
    "The `make_white_noise` function creates an image of the given size of random RGB levels, effectively producing a white noise image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cStringIO import StringIO\n",
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display\n",
    "from google.protobuf import text_format\n",
    "\n",
    "import caffe\n",
    "\n",
    "import random\n",
    "\n",
    "# Paths to network model and weight data\n",
    "model_path = 'models/bvlc_googlenet/'\n",
    "net_fn = model_path + 'deploy.prototxt'\n",
    "param_fn = model_path + 'bvlc_googlenet.caffemodel'\n",
    "\n",
    "# Functions\n",
    "def showarray(a, fmt='jpeg'):\n",
    "    \"\"\"Display an image defined in an array\"\"\"\n",
    "    \n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "\n",
    "# a couple of utility functions for converting to and from Caffe's input image layout\n",
    "def preprocess(net, img):\n",
    "    return np.float32(np.rollaxis(img, 2)[::-1]) - net.transformer.mean['data']\n",
    "\n",
    "def deprocess(net, img):\n",
    "    return np.dstack((img + net.transformer.mean['data'])[::-1])\n",
    "\n",
    "def make_white_noise_image(width, height):\n",
    "    \"\"\"Make a white noise image of the given width and height\"\"\"\n",
    "    \n",
    "    img = PIL.Image.new(\"RGBA\", (width, height), 'white')\n",
    "    noise_vals = map(lambda x: (\n",
    "            int(random.random() * 256),\n",
    "            int(random.random() * 256),\n",
    "            int(random.random() * 256)\n",
    "        ), [0] * width * height)\n",
    "    img.putdata(noise_vals)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model must be modified to allow for the gradients to be calculated required by the back-propagation step, which is not typically needed once the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Patching model to be able to compute gradients.\n",
    "# Note that you can also manually add \"force_backward: true\" line to \"deploy.prototxt\".\n",
    "proto_file = 'project_tmp.prototxt'\n",
    "\n",
    "model = caffe.io.caffe_pb2.NetParameter()\n",
    "text_format.Merge(open(net_fn).read(), model)\n",
    "model.force_backward = True\n",
    "open(proto_file, 'w').write(str(model))\n",
    "\n",
    "net = caffe.Classifier(proto_file, param_fn,\n",
    "                       mean = np.float32([104.0, 116.0, 122.0]), # ImageNet mean, training set dependent\n",
    "                       channel_swap = (2,1,0)) # the reference model has channels in BGR order instead of RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used to actually produce the images by performing the gradient ascent with a goal of maximizing certain activations of one of the layers in the network. In this example, taken from DeepDream, the L2-norm value is maximized during the back-propagation.\n",
    "\n",
    "The target layer is specified as the `end` parameter of the `make_step` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective_L2(dst):\n",
    "    dst.diff[:] = dst.data \n",
    "\n",
    "def make_step(net, step_size=1.5, end='inception_4c/output', \n",
    "              jitter=32, clip=True, objective=objective_L2):\n",
    "    '''Basic gradient ascent step.'''\n",
    "\n",
    "    src = net.blobs['data'] # input image is stored in Net's 'data' blob\n",
    "    dst = net.blobs[end]\n",
    "\n",
    "    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n",
    "    src.data[0] = np.roll(np.roll(src.data[0], ox, -1), oy, -2) # apply jitter shift\n",
    "            \n",
    "    net.forward(end=end)\n",
    "    objective(dst)  # specify the optimization objective\n",
    "    net.backward(start=end)\n",
    "    g = src.diff[0]\n",
    "    # apply normalized ascent step to the input image\n",
    "    src.data[:] += step_size/np.abs(g).mean() * g\n",
    "\n",
    "    src.data[0] = np.roll(np.roll(src.data[0], -ox, -1), -oy, -2) # unshift image\n",
    "            \n",
    "    if clip:\n",
    "        bias = net.transformer.mean['data']\n",
    "        src.data[:] = np.clip(src.data, -bias, 255-bias)\n",
    "        \n",
    "def deepdream(net, base_img, iter_n=10, octave_n=4, octave_scale=1.4, \n",
    "              end='inception_4c/output', clip=True, **step_params):\n",
    "    # prepare base images for all octaves\n",
    "    octaves = [preprocess(net, base_img)]\n",
    "    for i in xrange(octave_n-1):\n",
    "        octaves.append(nd.zoom(octaves[-1], (1, 1.0/octave_scale,1.0/octave_scale), order=1))\n",
    "    \n",
    "    src = net.blobs['data']\n",
    "    detail = np.zeros_like(octaves[-1]) # allocate image for network-produced details\n",
    "    for octave, octave_base in enumerate(octaves[::-1]):\n",
    "        h, w = octave_base.shape[-2:]\n",
    "        if octave > 0:\n",
    "            # upscale details from the previous octave\n",
    "            h1, w1 = detail.shape[-2:]\n",
    "            detail = nd.zoom(detail, (1, 1.0*h/h1,1.0*w/w1), order=1)\n",
    "\n",
    "        src.reshape(1,3,h,w) # resize the network's input image size\n",
    "        src.data[0] = octave_base+detail\n",
    "        for i in xrange(iter_n):\n",
    "            make_step(net, end=end, clip=clip, **step_params)\n",
    "            \n",
    "            # visualization\n",
    "            vis = deprocess(net, src.data[0])\n",
    "            if not clip: # adjust image contrast if clipping is disabled\n",
    "                vis = vis*(255.0/np.percentile(vis, 99.98))\n",
    "            showarray(vis)\n",
    "            print octave, i, end, vis.shape\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        # extract details produced on the current octave\n",
    "        detail = src.data[0]-octave_base\n",
    "    # returning the resulting image\n",
    "    return deprocess(net, src.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is chosen to start from. The original image of the Syracuse University quad is shown. \n",
    "\n",
    "This image is taken from the Syracuse University Financial Aid webpage at http://financialaid.syr.edu/whoareyou/internationalstudents/. Copyright Â© 2015. All Rights Reserved. Syracuse University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = np.float32(PIL.Image.open('su_quad.jpg'))\n",
    "showarray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the image is run through the model. By default, the image is processed through four rounds of 10 steps each. The original image is first down-sampled. In each round, the down-sampled image is zoomed from the previous round. The steps within each round actually perform the gradient ascent and modify the input image to maximize the targeted layer.\n",
    "\n",
    "When the example image is run, several features begin to appear in the modified image:\n",
    "\n",
    "- The yellow trees are beginning to represent images of some types of animals, as legs, faces and ears begin to appear.\n",
    "- To the right of Hendricks Chapel something that has the shape of a person, possibly with an eye appears.\n",
    "- The Carrier Dome, Hendricks Chapel itself, and the sky have taken on a surreal texture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_img = deepdream(net, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(np.uint8(out_img)).save(\"su_quad-inception_4c.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the same image is used, but a different layer is targeted, the results are completely different. Here an earlier layer is selected. The features that begin to appear are less concrete, as the earlier layers of a convolutional neural network activate for more abstract features, such as edges or corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=deepdream(net, img, end='inception_3b/5x5_reduce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if the process is done on the same image with a higher layer, features that are more concrete become prominent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=deepdream(net, img, end='inception_5a/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the image is run through the algorithm 100 times and the results produced are quite something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "# Make unique output directory for images\n",
    "out_dir = 'su_quad-{:%Y%m%dT%H%M%S}'.format(datetime.datetime.now())\n",
    "os.makedirs(out_dir)\n",
    "\n",
    "frame = img\n",
    "frame_i = 0\n",
    "\n",
    "h, w = frame.shape[:2]\n",
    "s = 0.05 # scale coefficient\n",
    "for i in xrange(100):\n",
    "    frame = deepdream(net, frame)\n",
    "    PIL.Image.fromarray(np.uint8(frame)).save(\"{}/{:04d}.jpg\".format(out_dir, frame_i))\n",
    "    frame = nd.affine_transform(frame, [1-s,1-s,1], [h*s/2,w*s/2,0], order=1)\n",
    "    frame_i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a white noise image is generated and then shown being input into the network, again targeting several different layers in the network. This demonstrates that the network even attempts to recognize and classify things when presented with a completely random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = np.float32(PIL.Image.open('su_quad.jpg'))\n",
    "# noise_img = make_white_noise_image(224, 224)\n",
    "noise_img = np.float32(PIL.Image.open('white_noise.jpeg'))\n",
    "showarray(noise_img)\n",
    "# noise_img.save('white_noise.jpeg')\n",
    "\n",
    "# Save image to memory file\n",
    "f = StringIO()\n",
    "noise_img.save(f, 'jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=deepdream(net, np.float32(PIL.Image.open(f)), octave_n=4, iter_n=10, end='inception_5a/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=deepdream(net, np.float32(PIL.Image.open(f)), octave_n=4, iter_n=10, end='inception_3a/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=deepdream(net, np.float32(PIL.Image.open(f)), octave_n=4, iter_n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
